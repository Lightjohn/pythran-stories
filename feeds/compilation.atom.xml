<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Pythran stories</title><link href="http://serge-sans-paille.github.io/pythran-stories/" rel="alternate"></link><link href="http://serge-sans-paille.github.io/pythran-stories/feeds/compilation.atom.xml" rel="self"></link><id>http://serge-sans-paille.github.io/pythran-stories/</id><updated>2016-03-29T00:00:00+02:00</updated><entry><title>Compiler Flags</title><link href="http://serge-sans-paille.github.io/pythran-stories/compiler-flags.html" rel="alternate"></link><updated>2016-03-29T00:00:00+02:00</updated><author><name>serge-sans-paille</name></author><id>tag:serge-sans-paille.github.io,2016-03-29:pythran-stories/compiler-flags.html</id><summary type="html">&lt;div class="section" id="when-size-matters"&gt;
&lt;h2&gt;When Size Matters&lt;/h2&gt;
&lt;p&gt;Everything started a few days ago with a Pythran user complaining about the
size of the binaries generated by Pythran. In essence, take the following code
&lt;cite&gt;cda.py&lt;/cite&gt;:&lt;/p&gt;
&lt;pre class="code python literal-block"&gt;
&lt;span class="c"&gt;#pythran export closest_distance_arrays(float, float, float[], float[])&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;math&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;closest_distance_arrays&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lat1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;long1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;latitudes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;longitudes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;degrees_to_radians&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mf"&gt;180.0&lt;/span&gt;
    &lt;span class="n"&gt;phi1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;90.0&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;lat1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;degrees_to_radians&lt;/span&gt;
    &lt;span class="n"&gt;phi2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;90.0&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;latitudes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;degrees_to_radians&lt;/span&gt;
    &lt;span class="n"&gt;theta1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;long1&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;degrees_to_radians&lt;/span&gt;
    &lt;span class="n"&gt;theta2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;longitudes&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;degrees_to_radians&lt;/span&gt;
    &lt;span class="n"&gt;cos&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;theta1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;theta2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
           &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;arc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arccos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;cos&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arc&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;arc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;It doesn't even weight a kilobyte, and when benchmarked, it runs in a few milliseconds:&lt;/p&gt;
&lt;pre class="code sh literal-block"&gt;
&amp;gt; python -m timeit -s &lt;span class="s1"&gt;'import numpy as np; n = 20000 ; lat, lon = np.random.rand(n), np.random.rand(n); x,y = np.random.rand(), np.random.rand(); from cda import closest_distance_arrays'&lt;/span&gt; &lt;span class="s1"&gt;'closest_distance_arrays(x,y,lat, lon)'&lt;/span&gt;
&lt;span class="m"&gt;100&lt;/span&gt; loops, best of 3: 1.95 msec per loop
&lt;/pre&gt;
&lt;p&gt;Thanks to the &lt;tt class="docutils literal"&gt;#pythran export&lt;/tt&gt; annotation, Pythran can turn it into a native
library that runs slightly faster than the Python version:&lt;/p&gt;
&lt;pre class="code sh literal-block"&gt;
&amp;gt; pythran cda.py
&amp;gt; python -m timeit -s &lt;span class="s1"&gt;'import numpy as np; n = 20000 ; lat, lon = np.random.rand(n), np.random.rand(n); x,y = np.random.rand(), np.random.rand(); from cda import closest_distance_arrays'&lt;/span&gt; &lt;span class="s1"&gt;'closest_distance_arrays(x,y,lat, lon)'&lt;/span&gt;
&lt;span class="m"&gt;1000&lt;/span&gt; loops, best of 3: 1.17 msec per loop
&lt;/pre&gt;
&lt;p&gt;It is, however, a very big binary:&lt;/p&gt;
&lt;pre class="code sh literal-block"&gt;
&amp;gt; ls -lh cda.so
-rwxr-xr-x &lt;span class="m"&gt;1&lt;/span&gt; sguelton sguelton 1.3M Mar &lt;span class="m"&gt;29&lt;/span&gt; 18:10 cda.so*
&lt;/pre&gt;
&lt;p&gt;Who wants to multiply the binary size by &lt;tt class="docutils literal"&gt;2e3&lt;/tt&gt; to get less than a &lt;tt class="docutils literal"&gt;x2&lt;/tt&gt; speedup?&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-culprits-debug-informations"&gt;
&lt;h2&gt;The culprits: Debug Informations&lt;/h2&gt;
&lt;p&gt;One can call Pythran with the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-v&lt;/span&gt;&lt;/tt&gt; flag to inspect part of its internal,
especially the C++ compiler call done to perform object code generation and
linking:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;
&amp;gt; pythran cda.py -v
running build_ext
running build_src
build_src
building extension &amp;quot;cda&amp;quot; sources
build_src: building npy-pkg config files
new_compiler returns distutils.unixccompiler.UnixCCompiler
INFO     customize UnixCCompiler
customize UnixCCompiler using build_ext
********************************************************************************
distutils.unixccompiler.UnixCCompiler
linker_exe    = ['gcc']
compiler_so   = ['gcc', '-DNDEBUG', '-g', '-fwrapv', '-O2', '-Wall', '-Wstrict-prototypes', '-fno-strict-aliasing', '-g', '-O2', '-fPIC']
archiver      = ['x86_64-linux-gnu-gcc-ar', 'rc']
preprocessor  = ['gcc', '-E']
linker_so     = ['x86_64-linux-gnu-gcc', '-pthread', '-shared', '-Wl,-O1', '-Wl,-Bsymbolic-functions', '-Wl,-z,relro', '-fno-strict-aliasing', '-DNDEBUG', '-g', '-fwrapv', '-O2', '-Wall', '-Wstrict-prototypes', '-Wdate-time', '-D_FORTIFY_SOURCE=2', '-g', '-fstack-protector-strong', '-Wformat', '-Werror=format-security', '-Wl,-z,relro', '-g', '-O2']
compiler_cxx  = ['g++']
ranlib        = None
compiler      = ['gcc', '-DNDEBUG', '-g', '-fwrapv', '-O2', '-Wall', '-Wstrict-prototypes', '-fno-strict-aliasing', '-g', '-O2']
libraries     = []
library_dirs  = []
include_dirs  = ['/usr/include/python2.7']
[...]
INFO     Generated module: cda
INFO     Output: /home/sguelton/sources/pythran/cda.so
&lt;/pre&gt;
&lt;p&gt;That's a pretty long trace, but that's what verbose mode is for. The
enlightened reader noticed that we use &lt;tt class="docutils literal"&gt;distutils&lt;/tt&gt; under the hood to abstract
the compiler calls, and that's why we're getting some funky compiler flags like
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-g&lt;/span&gt; &lt;span class="pre"&gt;-fwrapv&lt;/span&gt; &lt;span class="pre"&gt;-O2&lt;/span&gt; &lt;span class="pre"&gt;-Wall&lt;/span&gt; &lt;span class="pre"&gt;-fno-strict-aliasing&lt;/span&gt; &lt;span class="pre"&gt;-g&lt;/span&gt; &lt;span class="pre"&gt;-O2&lt;/span&gt; &lt;span class="pre"&gt;-fPIC&lt;/span&gt;&lt;/tt&gt; or even funkier
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-fstack-protector-strong&lt;/span&gt; &lt;span class="pre"&gt;-Wformat&lt;/span&gt; &lt;span class="pre"&gt;-Werror=format-security&lt;/span&gt; &lt;span class="pre"&gt;-Wl,-z,relro&lt;/span&gt;&lt;/tt&gt;.
That's the default for native python extensions on my distrib. Funny enough the
last ones are hardening flags used to improve the security of the binary and I
wrote a (passionating) article about it for Quarkslab &lt;a class="footnote-reference" href="#id4" id="id1"&gt;[0]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It turns out &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-g&lt;/span&gt;&lt;/tt&gt; (and C++) is responsible for the fat binary: if we simply
strip the binary, we get back to a decent size:&lt;/p&gt;
&lt;pre class="code sh literal-block"&gt;
&amp;gt; strip cda.so
&amp;gt; ls -lh cda.so
-rwxr-xr-x &lt;span class="m"&gt;1&lt;/span&gt; sguelton sguelton 151K Mar &lt;span class="m"&gt;29&lt;/span&gt; 18:26 cda.so
&lt;/pre&gt;
&lt;p&gt;As Pythran users generally don't want the debug info on the generated native
code, we chose to strip them by default, using the linker flag
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-Wl,-strip-all&lt;/span&gt;&lt;/tt&gt; that removes all symbol informations, including debug
symbols.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="a-step-further-default-symbol-visibility"&gt;
&lt;h2&gt;A Step further: Default Symbol visibility&lt;/h2&gt;
&lt;p&gt;While we're at it, let's call &lt;tt class="docutils literal"&gt;nm&lt;/tt&gt; to check if any symbol remains in the
binary. After all, the Python interpreter still needs some of them to load the
native extension!&lt;/p&gt;
&lt;pre class="code sh literal-block"&gt;
&amp;gt; nm -C -D cda.so
&lt;span class="o"&gt;[&lt;/span&gt;...&lt;span class="o"&gt;]&lt;/span&gt; skipping &amp;gt; &lt;span class="m"&gt;900&lt;/span&gt; entries
000000000001ed00 u nt2::ext::implement&amp;lt;nt2::tag::rem_pio2_ &lt;span class="o"&gt;(&lt;/span&gt;boost::dispatch::meta::scalar_&amp;lt;boost::dispatch::meta::double_&amp;lt;double&amp;gt; &amp;gt;, boost::dispatch::meta::scalar_&amp;lt;boost::dispatch::meta::double_&amp;lt;double&amp;gt; &amp;gt;, boost::dispatch::meta::scalar_&amp;lt;boost::dispatch::meta::double_&amp;lt;double&amp;gt; &amp;gt;&lt;span class="o"&gt;)&lt;/span&gt;, boost::dispatch::tag::cpu_, void&amp;gt;::__kernel_rem_pio2&lt;span class="o"&gt;(&lt;/span&gt;double*, double*, int, int, int, int const*&lt;span class="o"&gt;)&lt;/span&gt;::PIo2
000000000001edc0 u nt2::ext::implement&amp;lt;nt2::tag::rem_pio2_ &lt;span class="o"&gt;(&lt;/span&gt;boost::dispatch::meta::scalar_&amp;lt;boost::dispatch::meta::double_&amp;lt;double&amp;gt; &amp;gt;, boost::dispatch::meta::scalar_&amp;lt;boost::dispatch::meta::double_&amp;lt;double&amp;gt; &amp;gt;, boost::dispatch::meta::scalar_&amp;lt;boost::dispatch::meta::double_&amp;lt;double&amp;gt; &amp;gt;&lt;span class="o"&gt;)&lt;/span&gt;, boost::dispatch::tag::cpu_, void&amp;gt;::__ieee754_rem_pio2&lt;span class="o"&gt;(&lt;/span&gt;double, double*&lt;span class="o"&gt;)&lt;/span&gt;::two_over_pi
000000000001ed40 u nt2::ext::implement&amp;lt;nt2::tag::rem_pio2_ &lt;span class="o"&gt;(&lt;/span&gt;boost::dispatch::meta::scalar_&amp;lt;boost::dispatch::meta::double_&amp;lt;double&amp;gt; &amp;gt;, boost::dispatch::meta::scalar_&amp;lt;boost::dispatch::meta::double_&amp;lt;double&amp;gt; &amp;gt;, boost::dispatch::meta::scalar_&amp;lt;boost::dispatch::meta::double_&amp;lt;double&amp;gt; &amp;gt;&lt;span class="o"&gt;)&lt;/span&gt;, boost::dispatch::tag::cpu_, void&amp;gt;::__ieee754_rem_pio2&lt;span class="o"&gt;(&lt;/span&gt;double, double*&lt;span class="o"&gt;)&lt;/span&gt;::npio2_hw
&lt;/pre&gt;
&lt;p&gt;I can tell you Python is &lt;em&gt;not&lt;/em&gt; using nt2 dispatch mechanism to load native
extensions. Again, the default compiler settings are responsible for this
noise, and the relevant compiler flag is &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-fvisibility=hidden&lt;/span&gt;&lt;/tt&gt; that tells the
compiler than only the functions flagged with a special attribute are part of
the external ABI, the other ones are not exported. As Python uses a single
entry point to load Pythran modules, namely &lt;tt class="docutils literal"&gt;PyInit_cda&lt;/tt&gt; for Python3 modules
and &lt;tt class="docutils literal"&gt;initcda&lt;/tt&gt; for Python2 modules &lt;a class="footnote-reference" href="#id5" id="id2"&gt;[1]&lt;/a&gt;, one can add the &lt;tt class="docutils literal"&gt;__attribute__
&lt;span class="pre"&gt;((visibility(&amp;quot;default&amp;quot;)))&lt;/span&gt;&lt;/tt&gt; on this symbol and it will be the only exported
one. This slightly impacts the code size, may decrease loading time and
eventually gives the compiler more optimization opportunities, but nothing
significant there (131K), apart the pleasure of generating cleaner binaries.
That's also going to be the default for next Pythran version.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="out-of-chance-getting-faster-binaries"&gt;
&lt;h2&gt;Out of chance: getting faster binaries&lt;/h2&gt;
&lt;p&gt;In the (huge) info pages of GCC, near the doc of &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-fvisibility=hidden&lt;/span&gt;&lt;/tt&gt;,
there's this (GCC only) compiler flag, &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-fwhole-program&lt;/span&gt;&lt;/tt&gt; that implements some
kind of Link Time Optimization, in the sense that it tells the compiler to
consider the current compilation unit (or code) as a whole program. As
specified in the GCC man page, &amp;quot;All public functions and variables with the
exception of &amp;quot;main&amp;quot; and those merged by attribute &amp;quot;externally_visible&amp;quot; become
static functions and in effect are optimized more aggressively by
interprocedural optimizers.&amp;quot;, which basically means that every function is
considered static except for &amp;quot;main&amp;quot; and the ones that are explicitly told not
to be.  This allows the compiler for instance to remove functions that are
always inlined, and thus win space. So we flag the &lt;tt class="docutils literal"&gt;initcda&lt;/tt&gt; function with
&lt;tt class="docutils literal"&gt;__attribute__ ((externally_visible))&lt;/tt&gt;. That sounds a bit redundant to me
with the visibility attribute, but it turns out this triggers abunch of
different optimization path that gives us a significantly smaller binary, that
runs slightly faster:&lt;/p&gt;
&lt;pre class="code sh literal-block"&gt;
&amp;gt; pythran cda.py -fvisibility&lt;span class="o"&gt;=&lt;/span&gt;hidden -fwhole-program -Wl,-strip-all
&amp;gt; ls -lh cda.so
-rwxr-xr-x &lt;span class="m"&gt;1&lt;/span&gt; sguelton sguelton 31K Mar &lt;span class="m"&gt;29&lt;/span&gt; 18:52 cda.so*
&amp;gt; python -m timeit -s &lt;span class="s1"&gt;'import numpy as np; n = 20000 ; lat, lon = np.random.rand(n), np.random.rand(n); x,y = np.random.rand(), np.random.rand(); from cda import closest_distance_arrays'&lt;/span&gt; &lt;span class="s1"&gt;'closest_distance_arrays(x,y,lat, lon)'&lt;/span&gt;
&lt;span class="m"&gt;1000&lt;/span&gt; loops, best of 3: 1.15 msec per loop
&lt;/pre&gt;
&lt;p&gt;All these flags are now the default on Linux.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="playing-with-the-optimization-flags-too"&gt;
&lt;h2&gt;Playing with the optimization flags too&lt;/h2&gt;
&lt;p&gt;The default optimization flag is &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-O2&lt;/span&gt;&lt;/tt&gt;, and that's generally a decent choice.
On &lt;tt class="docutils literal"&gt;cda.py&lt;/tt&gt;, using &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-O3&lt;/span&gt;&lt;/tt&gt; does not give much change (gcc 4.9):&lt;/p&gt;
&lt;pre class="code sh literal-block"&gt;
&amp;gt; pythran cda.py -fvisibility&lt;span class="o"&gt;=&lt;/span&gt;hidden -fwhole-program -Wl,-strip-all -O3
&amp;gt; python -m timeit &lt;span class="o"&gt;[&lt;/span&gt;...&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="m"&gt;1000&lt;/span&gt; loops, best of 3: 1.14 msec per loop
&lt;/pre&gt;
&lt;p&gt;Asking for code specific to my CPU using &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-march=native&lt;/span&gt;&lt;/tt&gt; actually gives some improvments&lt;/p&gt;
&lt;pre class="code sh literal-block"&gt;
&amp;gt; pythran cda.py -fvisibility&lt;span class="o"&gt;=&lt;/span&gt;hidden -fwhole-program -Wl,-strip-all -O3 -march&lt;span class="o"&gt;=&lt;/span&gt;native
&amp;gt; python -m timeit &lt;span class="o"&gt;[&lt;/span&gt;...&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="m"&gt;1000&lt;/span&gt; loops, best of 3: 1.11 msec per loop
&lt;/pre&gt;
&lt;p&gt;But the best speedup has a price: relaxing standard compliance with &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-Ofast&lt;/span&gt;&lt;/tt&gt;
can be beneficial if you're not using denormalized numbers, infinity and the
monstrosity that lies with &lt;tt class="docutils literal"&gt;NaN&lt;/tt&gt;:&lt;/p&gt;
&lt;pre class="code sh literal-block"&gt;
&amp;gt; pythran cda.py -fvisibility&lt;span class="o"&gt;=&lt;/span&gt;hidden -fwhole-program -Wl,-strip-all -Ofast -march&lt;span class="o"&gt;=&lt;/span&gt;native
&amp;gt; python -m timeit &lt;span class="o"&gt;[&lt;/span&gt;...&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="m"&gt;1000&lt;/span&gt; loops, best of 3: 1.02 msec per loop
&lt;/pre&gt;
&lt;p&gt;If you're really into compiler flags tuning, you can try out &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-funroll-loops&lt;/span&gt;&lt;/tt&gt;
or try to tune the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-finline-limit=N&lt;/span&gt;&lt;/tt&gt; parameter (that actually get mets dow
to &lt;tt class="docutils literal"&gt;1ms per loop&lt;/tt&gt;) but that's going a bit too far :-)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="don-t-forget-vectorization"&gt;
&lt;h2&gt;Don't forget Vectorization&lt;/h2&gt;
&lt;p&gt;Combining &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-O3&lt;/span&gt;&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-march=native&lt;/span&gt;&lt;/tt&gt; triggers compiler auto-vectorization[2]_,
but that did not helped much on our case. Indeed, automatic vectorization, as
in « I am using the multimedia instruction set of my CPU » is still a difficult
task for compilers. Fortunately Pythran helps here, and passing the
not-so-experimental-anymore-but-still-not-default flag &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-DUSE_BOOST_SIMD&lt;/span&gt;&lt;/tt&gt;
triggers some hard-coded vectorization based on &lt;tt class="docutils literal"&gt;boost.simd&lt;/tt&gt; &lt;a class="footnote-reference" href="#id7" id="id3"&gt;[3]&lt;/a&gt;, and that
&lt;strong&gt;did&lt;/strong&gt; help:&lt;/p&gt;
&lt;pre class="code sh literal-block"&gt;
&amp;gt; &lt;span class="c"&gt;# esod mumixam
&lt;/span&gt;&amp;gt; python -m pythran.run cda.cpp -fvisibility&lt;span class="o"&gt;=&lt;/span&gt;hidden -fwhole-program -Wl,-strip-all -Ofast -march&lt;span class="o"&gt;=&lt;/span&gt;native -funroll-loops -finline-limit&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;100000000&lt;/span&gt; -DUSE_BOOST_SIMD
&amp;gt; python -m timeit &lt;span class="o"&gt;[&lt;/span&gt;...&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="m"&gt;1000&lt;/span&gt; loops, best of 3: &lt;span class="m"&gt;462&lt;/span&gt; usec per loo
&lt;/pre&gt;
&lt;p&gt;And that's woth 63 kilobytes :-)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="concluding-remarks"&gt;
&lt;h2&gt;Concluding Remarks&lt;/h2&gt;
&lt;p&gt;Source-to-source compilers &lt;em&gt;do&lt;/em&gt; generate ugly intermediate code, and Pythran is
not an exception. One benefit though is that you can get a full control over
the &lt;em&gt;backend&lt;/em&gt; compiler, which means you can tune it to your needs. Given some
knowledge and benchmarking effort, it can get you closer to your goal without
changing the original code.&lt;/p&gt;
&lt;table class="docutils footnote" frame="void" id="id4" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id1"&gt;[0]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;And I am shamelessly advertising it :-) &lt;a class="reference external" href="http://blog.quarkslab.com/clang-hardening-cheat-sheet.html"&gt;http://blog.quarkslab.com/clang-hardening-cheat-sheet.html&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id5" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id2"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;If you really want to inspect the intermediate C++ code generated by pythran use the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-E&lt;/span&gt;&lt;/tt&gt; flag and a &lt;tt class="docutils literal"&gt;cda.cpp&lt;/tt&gt; will be generated.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id6" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;[2]&lt;/td&gt;&lt;td&gt;only GCC needs this, clang turns vectorisation at &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-O2&lt;/span&gt;&lt;/tt&gt;. &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-march=native&lt;/span&gt;&lt;/tt&gt; allows it to use a more recent instruction set if available.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id7" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id3"&gt;[3]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Thanks Numscale &lt;a class="reference external" href="https://www.numscale.com/boost-simd/"&gt;https://www.numscale.com/boost-simd/&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</summary></entry></feed>